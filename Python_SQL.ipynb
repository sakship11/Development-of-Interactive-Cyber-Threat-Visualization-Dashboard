{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt 1: Load and Describe Dataset\n",
        "\n",
        "**Prompt:**  \n",
        "Load the dataset by uploading an external CSV file and describe the dataset using\n",
        "head(), tail(), info(), and describe().\n",
        "\n"
      ],
      "metadata": {
        "id": "j4qgJuK3AQ-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Upload CSV file\n",
        "print(\"Please upload your CSV file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read CSV into DataFrame\n",
        "for file_name in uploaded.keys():\n",
        "    print(f'User uploaded file \"{file_name}\"')\n",
        "    df = pd.read_csv(io.StringIO(uploaded[file_name].decode('utf-8')))\n",
        "\n",
        "print(\"Dataset loaded successfully into DataFrame 'df'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "O76dJaNNAWaS",
        "outputId": "6a5fac68-3e81-479b-c740-24bcc5ee7e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your CSV file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cef3d03d-29d2-4eb7-bebf-a4e849b2647d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cef3d03d-29d2-4eb7-bebf-a4e849b2647d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "IxBH_0aVAuhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "vd47JBw3BAKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "CVNIhkyMBDC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "9cYP1bZdBF1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT 2 Data Cleaning and Feature Engineering for Machine Learning"
      ],
      "metadata": {
        "id": "4SIckSGmBX8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:\n",
        "\n",
        "Perform data cleaning on the dataset to prepare it for machine learning.\n",
        "The process should include:\n",
        "\n",
        "Checking and quantifying missing values\n",
        "\n",
        "Identifying and removing duplicate rows\n",
        "\n",
        "Converting the timestamp column to a datetime object\n",
        "\n",
        "Summarizing the cleaning steps and the current state of the dataset"
      ],
      "metadata": {
        "id": "-0tq0t5CBivG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify and quantify any missing values across all columns in the DataFrame to understand data completeness.\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "To assess data quality, it is necessary to identify missing values that may impact model performance.\n",
        "This is achieved by calculating the total number of null values for each column in the DataFrame df."
      ],
      "metadata": {
        "id": "QCwvEOA2CG0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "Oyv6gHPnBqF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subtask 2: Remove Duplicate Rows\n",
        "Objective:\n",
        "\n",
        "Identify and remove duplicate records to avoid biased or repeated data in machine learning models.\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "Duplicate rows can distort model training by over-representing certain data points.\n",
        "Removing duplicates ensures data integrity and improves model reliability."
      ],
      "metadata": {
        "id": "2ldbgzOECJvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows before removal: {duplicate_count}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"Duplicate rows removed successfully.\")\n"
      ],
      "metadata": {
        "id": "UP6P-5JWB5jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subtask 3: Convert Timestamp Column to Datetime\n",
        "Objective:\n",
        "\n",
        "Ensure the timestamp column is in datetime format for time-based analysis and feature extraction.\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "Machine learning models and time-series analysis require timestamp data to be in a proper datetime format.\n",
        "This allows extraction of features such as year, month, day, and hour."
      ],
      "metadata": {
        "id": "ryPf3isqCRVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp column to datetime format\n",
        "# Convert order_date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "print(\"date column converted to datetime format.\")\n",
        "display(df.head())\n",
        "df.info()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-w4uBWkiCUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 3: Use Machine Learning Models"
      ],
      "metadata": {
        "id": "rlKhzeAbF3de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different machine learning algorithms capture different patterns in data.\n",
        "Direct models learn independently, while hybrid models combine multiple\n",
        "algorithms to improve prediction accuracy, robustness, and generalization.\n",
        "Using both types allows comprehensive performance comparison.\n"
      ],
      "metadata": {
        "id": "Tbijttt6Go_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n"
      ],
      "metadata": {
        "id": "tFW6Dw7KGq4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['financial_loss_usd', 'incident_id'])\n",
        "y = df['financial_loss_usd']\n",
        "\n"
      ],
      "metadata": {
        "id": "edj6yJAJG3DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "jB98RHy5G8PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "0tlf6fFKG_xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: Linear Regression\n",
        "\n",
        "Type: Regression\n",
        "\n",
        "Purpose: Predicts continuous values (e.g., revenue)\n",
        "\n",
        "Why used: Simple baseline model to understand linear relationships"
      ],
      "metadata": {
        "id": "NOgOj-fdHHrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "pred_lr = lr.predict(X_test)\n",
        "\n",
        "print(\"Linear Regression R2:\", r2_score(y_test, pred_lr))\n"
      ],
      "metadata": {
        "id": "9FvejzajHOYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: Ridge Regression\n",
        "\n",
        "Type: Regularized Regression\n",
        "\n",
        "Purpose: Handles multicollinearity using L2 regularization\n",
        "\n",
        "Why used: Improves stability over Linear Regression"
      ],
      "metadata": {
        "id": "Ek5_x_Y3HTyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', Ridge(alpha=1.0))\n",
        "])\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "print(\"Ridge Regression R2:\", r2_score(y_test, pred_ridge))\n"
      ],
      "metadata": {
        "id": "qU3cwqvtHXFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3: Support Vector Regression (SVR)\n",
        "\n",
        "Type: Kernel-based Model\n",
        "\n",
        "Purpose: Captures complex non-linear patterns\n",
        "\n",
        "Why used: Effective for high-dimensional data"
      ],
      "metadata": {
        "id": "K_0rAfHJHotL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svr = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', SVR(kernel='rbf'))\n",
        "])\n",
        "\n",
        "svr.fit(X_train, y_train)\n",
        "pred_svr = svr.predict(X_test)\n",
        "\n",
        "print(\"SVR R2:\", r2_score(y_test, pred_svr))\n"
      ],
      "metadata": {
        "id": "yd8imCAZHvQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 4: K-Nearest Neighbors (KNN) Regressor\n",
        "\n",
        "Type: Instance-based Learning\n",
        "\n",
        "Purpose: Predicts values based on nearest data points\n",
        "\n",
        "Why used: Simple non-parametric approach for comparison"
      ],
      "metadata": {
        "id": "eZqkuFZQHwgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', KNeighborsRegressor(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "pred_knn = knn.predict(X_test)\n",
        "\n",
        "print(\"KNN R2:\", r2_score(y_test, pred_knn))\n"
      ],
      "metadata": {
        "id": "VJ6r-86uHy3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 5: Voting Regressor (Hybrid Model 1)\n",
        "\n",
        "Combination: Linear Regression + Decision Tree + Random Forest\n",
        "\n",
        "Purpose: Aggregates predictions using averaging\n",
        "\n",
        "Why used: Improves overall prediction stability"
      ],
      "metadata": {
        "id": "lM9EMrEbH8Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('lr', LinearRegression()),\n",
        "        ('dt', DecisionTreeRegressor(random_state=42)),\n",
        "        ('rf', RandomForestRegressor(n_estimators=50, random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "voting_model = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', voting)\n",
        "])\n",
        "\n",
        "voting_model.fit(X_train, y_train)\n",
        "pred_voting = voting_model.predict(X_test)\n",
        "\n",
        "print(\"Voting Regressor R2:\", r2_score(y_test, pred_voting))\n"
      ],
      "metadata": {
        "id": "7tuiJu97IDfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 6: Boosting Model (Gradient Boosting / XGBoost-like)\n",
        "\n",
        "Type: Sequential Hybrid Model\n",
        "\n",
        "Purpose: Corrects previous model errors iteratively\n",
        "\n",
        "Why used: High accuracy on structured sales data"
      ],
      "metadata": {
        "id": "7LxAGjmMIamG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gboost = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "gboost.fit(X_train, y_train)\n",
        "pred_gb = gboost.predict(X_test)\n",
        "\n",
        "print(\"Gradient Boosting R2:\", r2_score(y_test, pred_gb))\n"
      ],
      "metadata": {
        "id": "svKovzUoIWUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"Linear Regression\",\n",
        "        \"Ridge\",\n",
        "        \"SVR\",\n",
        "        \"KNN\",\n",
        "        \"Voting Regressor\",\n",
        "        \"Gradient Boosting\"\n",
        "    ],\n",
        "    \"R2 Score\": [\n",
        "        r2_score(y_test, pred_lr),\n",
        "        r2_score(y_test, pred_ridge),\n",
        "        r2_score(y_test, pred_svr),\n",
        "        r2_score(y_test, pred_knn),\n",
        "        r2_score(y_test, pred_voting),\n",
        "        r2_score(y_test, pred_gb)\n",
        "    ]\n",
        "}).round(3)\n",
        "\n",
        "results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DYGsGSYIfnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 4: Visualization of Model Performance\n",
        "\n",
        "Prompt:\n",
        "Create visual representations (bar graphs and line graphs) for the performance\n",
        "of each machine learning model. Additionally, generate comparative charts\n",
        "to analyze the performance of normal models versus hybrid models and\n",
        "identify which category performs better for detecting patterns in the dataset."
      ],
      "metadata": {
        "id": "ZL-wnIcDJPRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kcy2rU0rk3zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
        "\n",
        "print(df[['date']].head())\n"
      ],
      "metadata": {
        "id": "nLzy1i6Ao8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date is datetime\n",
        "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
        "\n",
        "# Create month column\n",
        "df['month'] = df['date'].dt.to_period('M')\n",
        "\n",
        "# Count incidents per month\n",
        "monthly_incidents = df.groupby('month').size()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "monthly_incidents.plot(marker='o')\n",
        "\n",
        "plt.title('Monthly Cybersecurity Incidents')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EtVvriVypAzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "severity_counts = df['severity'].value_counts()\n",
        "\n",
        "plt.bar(severity_counts.index, severity_counts.values)\n",
        "\n",
        "plt.title('Incident Severity Distribution')\n",
        "plt.xlabel('Severity Level')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iTEz21kopeIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_attacks = df['attack_type'].value_counts().head(5)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(top_attacks.index, top_attacks.values)\n",
        "\n",
        "plt.title('Top 5 Cyber Attack Types')\n",
        "plt.xlabel('Attack Type')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O0fiMs1TphHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_loss = df.groupby('severity')['financial_loss_usd'].mean()\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "avg_loss.plot(kind='bar')\n",
        "\n",
        "plt.title('Average Financial Loss by Severity')\n",
        "plt.xlabel('Severity')\n",
        "plt.ylabel('Average Loss (USD)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Egga_FEppj2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Queries\n"
      ],
      "metadata": {
        "id": "ZkHyb26FrFCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load DataFrame into In-Memory SQLite\n",
        "Load dataset into SQL database\n",
        "\n",
        "Prompt:\n",
        "Load the cybersecurity incidents DataFrame into an in-memory SQLite database so that SQL queries can be executed for analytical purposes.\n",
        "\n",
        "Reasoning:\n",
        "SQLite allows SQL-based analysis directly on pandas DataFrames without external database setup."
      ],
      "metadata": {
        "id": "UkgFNzXRrRNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "for col in df.columns:\n",
        "    if isinstance(df[col].dtype, pd.PeriodDtype):\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "for col in df.columns:\n",
        "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "print(\"Column types cleaned\")\n"
      ],
      "metadata": {
        "id": "4b-GRtGerTKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(':memory:')\n",
        "\n",
        "df.to_sql('incidents', conn, index=False, if_exists='replace')\n",
        "\n",
        "print(\"Table 'incidents' created successfully\")\n"
      ],
      "metadata": {
        "id": "z1uTPPzesCGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sql(query):\n",
        "    return pd.read_sql_query(query, conn)\n",
        "\n"
      ],
      "metadata": {
        "id": "eqbuZeyFsElc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Query 1: Total Incident Count\n",
        "\n",
        "Prompt:\n",
        "Find the total number of incidents recorded in the dataset."
      ],
      "metadata": {
        "id": "YmtsGsYIseAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sql(query):\n",
        "    return pd.read_sql_query(query, conn)\n"
      ],
      "metadata": {
        "id": "wEBSaKRTs3GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 1: Total Incidents"
      ],
      "metadata": {
        "id": "RQQYy7OVuxry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"\"\"\n",
        "SELECT COUNT(*) AS total_incidents\n",
        "FROM incidents;\n",
        "\"\"\"\n",
        "run_sql(query1)\n"
      ],
      "metadata": {
        "id": "0YMzvHZTtxDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 2: Incidents by Severity"
      ],
      "metadata": {
        "id": "7ik-vWUhu07i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"\"\"\n",
        "SELECT severity, COUNT(*) AS incident_count\n",
        "FROM incidents\n",
        "GROUP BY severity\n",
        "ORDER BY incident_count DESC;\n",
        "\"\"\"\n",
        "run_sql(query2)"
      ],
      "metadata": {
        "id": "XH9bOhGNuqsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 3: Incidents by Attack Type"
      ],
      "metadata": {
        "id": "UwQtSAeJu0J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"\"\"\n",
        "SELECT attack_type, COUNT(*) AS incident_count\n",
        "FROM incidents\n",
        "GROUP BY attack_type\n",
        "ORDER BY incident_count DESC;\n",
        "\"\"\"\n",
        "run_sql(query3)\n"
      ],
      "metadata": {
        "id": "UFFWVmTLutWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 4: Total Records Affected by Attack Type"
      ],
      "metadata": {
        "id": "tMaoEINou58L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query4 = \"\"\"\n",
        "SELECT attack_type, SUM(records_affected) AS total_records\n",
        "FROM incidents\n",
        "GROUP BY attack_type\n",
        "ORDER BY total_records DESC;\n",
        "\"\"\"\n",
        "run_sql(query4)\n"
      ],
      "metadata": {
        "id": "BcaqdF9mu8Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 5: Total Financial Loss by Industry\n"
      ],
      "metadata": {
        "id": "U1sFtZVevAVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query5 = \"\"\"\n",
        "SELECT industry, SUM(financial_loss_usd) AS total_loss\n",
        "FROM incidents\n",
        "GROUP BY industry\n",
        "ORDER BY total_loss DESC;\n",
        "\"\"\"\n",
        "run_sql(query5)\n"
      ],
      "metadata": {
        "id": "NE5K9LVevD0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 6: Top 5 Attack Detection Methods"
      ],
      "metadata": {
        "id": "xkJwtsiKvJdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query6 = \"\"\"\n",
        "SELECT attack_detected_by, COUNT(*) AS count\n",
        "FROM incidents\n",
        "GROUP BY attack_detected_by\n",
        "ORDER BY count DESC\n",
        "LIMIT 5;\n",
        "\"\"\"\n",
        "run_sql(query6)\n"
      ],
      "metadata": {
        "id": "BphPLLcGvMD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 7: Incidents Under Investigation vs Resolved vs Mitigated\n"
      ],
      "metadata": {
        "id": "1kICLVVUvPPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query7 = \"\"\"\n",
        "SELECT status, COUNT(*) AS count\n",
        "FROM incidents\n",
        "GROUP BY status;\n",
        "\"\"\"\n",
        "run_sql(query7)\n"
      ],
      "metadata": {
        "id": "DaozQ7jUvSMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 8: High Severity Incidents with Resolution Time > 150 hours"
      ],
      "metadata": {
        "id": "krSPHjl7vT_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query8 = \"\"\"\n",
        "SELECT incident_id, severity, resolution_time_hours, status\n",
        "FROM incidents\n",
        "WHERE severity IN ('High', 'Critical') AND resolution_time_hours > 150\n",
        "ORDER BY resolution_time_hours DESC;\n",
        "\"\"\"\n",
        "run_sql(query8)\n"
      ],
      "metadata": {
        "id": "68wADN_3vYKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 9: Incidents with High Financial Loss > $4,000,000\n"
      ],
      "metadata": {
        "id": "C78rhAaDvbCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query10 = \"\"\"\n",
        "SELECT incident_id, industry, attack_type, severity, status\n",
        "FROM incidents\n",
        "WHERE severity = 'Low' AND status != 'Mitigated'\n",
        "ORDER BY incident_id;\n",
        "\"\"\"\n",
        "run_sql(query10)\n",
        "\n"
      ],
      "metadata": {
        "id": "bMyYLmA7vdif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 10: Low Severity, Non-Mitigated Incidents\n"
      ],
      "metadata": {
        "id": "4IjkOFFTvgPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query10 = \"\"\"\n",
        "SELECT incident_id, industry, attack_type, severity, status\n",
        "FROM incidents\n",
        "WHERE severity = 'Low' AND status != 'Mitigated'\n",
        "ORDER BY incident_id;\n",
        "\"\"\"\n",
        "run_sql(query10)\n"
      ],
      "metadata": {
        "id": "FxRAamEOviyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}